{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0307cacf-6275-4a62-87a9-1885e28e2fd3",
   "metadata": {},
   "source": [
    "# U.S. Geological Survey Class GW3099\n",
    "Advanced Modeling of Groundwater Flow (GW3099)\\\n",
    "Boise, Idaho\\\n",
    "September 16 - 20, 2024\n",
    "\n",
    "![title](../../images/ClassLocation.jpg)\n",
    "\n",
    "# Multi-process models in pywatershed\n",
    "*(Note that this notebook follows the notebook in the pywatershed repository [examples/01_multi-process_models.ipynb](https://github.com/EC-USGS/pywatershed/blob/develop/examples/01_multi-process_models.ipynb) but it deviates in some of the details covered.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e142cbc-9b4b-44d1-8991-f98a57243b08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "In notebook [`step1_processes.ipynb`](step1_processes.ipynb), we looked at how individual Process representations work and are designed. In this notebook we learn how to put multiple `Processes` together into composite models using the `Model` class. \n",
    "\n",
    "The starting point for the development of `pywatershed` was the National Hydrologic Model (NHM, Regan et al., 2018) configuration of the Precipitation-Runoff Modeling System (PRMS, Regan et al., 2015). In this notebook, we'll first construct a full NHM configuration. We will again use the spatial domain of the Delaware River Basin. Once we construct the full NHM, we'll look at how we can also construct sub-models of the NHM.\n",
    "\n",
    "Along the way, we'll get into some of the guts of using pywatershed.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5fb06-f73f-489b-9eb8-760e7f5fee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "from platform import processor\n",
    "from pprint import pprint\n",
    "from sys import platform\n",
    "\n",
    "import hvplot.xarray  # noqa\n",
    "import jupyter_black\n",
    "import numpy as np\n",
    "import pywatershed as pws\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from helpers import do_not_run_this_cell, help_head, read_yaml, write_yaml\n",
    "from pywatershed.utils import gis_files\n",
    "from pywatershed.utils.path import dict_pl_to_str\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "jupyter_black.load()  # auto-format the code in this notebook\n",
    "\n",
    "pws.utils.gis_files.download()\n",
    "\n",
    "pkg_root_dir = pws.constants.__pywatershed_root__\n",
    "repo_root_dir = pkg_root_dir.parent\n",
    "\n",
    "nb_output_dir = pl.Path(\"./step2_multi-process_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424718e-8512-4e53-b0e6-587a20639d90",
   "metadata": {},
   "source": [
    "## Domain Plot to get to know the area\n",
    "\n",
    "Before diving in to pywatershed models, let's use one of its built-in tools to get familiar with the application domain. We'll combine the GIS files for the HRUs and the Segments in this domain with their parameters to learn more about how the model represents quantities in pyhiscal space. Please zoom in and out and select different layers. We aim to add more functionality to this plot over time, stay tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a0f3d-4583-4a96-844d-bbbeb652d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dir = pkg_root_dir / \"data/drb_2yr\"\n",
    "\n",
    "domain_gis_dir = pkg_root_dir / \"data/pywatershed_gis/drb_2yr\"\n",
    "shp_file_hru = domain_gis_dir / \"HRU_subset.shp\"\n",
    "shp_file_seg = domain_gis_dir / \"Segments_subset.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68904b2-640e-4b92-8723-87ff94c3efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_hru = pws.Parameters.from_netcdf(domain_dir / \"parameters_dis_hru.nc\")\n",
    "start_lat = dis_hru.parameters[\"hru_lat\"].mean()\n",
    "start_lon = dis_hru.parameters[\"hru_lon\"].mean()\n",
    "\n",
    "pws.plot.DomainPlot(\n",
    "    hru_shp_file=shp_file_hru,\n",
    "    segment_shp_file=shp_file_seg,\n",
    "    hru_parameters=domain_dir / \"parameters_dis_hru.nc\",\n",
    "    hru_parameter_names=[\n",
    "        \"nhm_id\",\n",
    "        \"hru_lat\",\n",
    "        \"hru_lon\",\n",
    "        \"hru_area\",\n",
    "    ],\n",
    "    segment_parameters=domain_dir / \"parameters_dis_seg.nc\",\n",
    "    segment_parameter_names=[\n",
    "        \"nhm_seg\",\n",
    "        \"seg_length\",\n",
    "        \"seg_slope\",\n",
    "        \"seg_cum_area\",\n",
    "    ],\n",
    "    start_lat=start_lat,\n",
    "    start_lon=start_lon,\n",
    "    start_zoom=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8896bb-96bc-4879-a2d5-38d083b33edd",
   "metadata": {},
   "source": [
    "## An NHM multi-process model for the Delaware River Basin\n",
    "The 8 conceptual `Process` classes that comprise the NHM are, in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529247fe-7f0b-4783-90a4-a4ddf55c1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhm_processes = [\n",
    "    pws.PRMSSolarGeometry,\n",
    "    pws.PRMSAtmosphere,\n",
    "    pws.PRMSCanopy,\n",
    "    pws.PRMSSnow,\n",
    "    pws.PRMSRunoff,\n",
    "    pws.PRMSSoilzone,\n",
    "    pws.PRMSGroundwater,\n",
    "    pws.PRMSChannel,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdde067-5032-443d-9c7c-6fc0340f7be8",
   "metadata": {},
   "source": [
    "We'll use this list of classes shortly to construct the NHM.\n",
    "\n",
    "A multi-process model is assembled by the `Model` class. We can take a quick look at the first 22 lines of help on `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bfbf7-00e0-4642-b7a6-dc06eee5722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help_head(pws.Model, n=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfb5e1-adf4-4eef-949a-90476e00b9f2",
   "metadata": {},
   "source": [
    "The `help()` mentions that there are 2 distinct ways of instantiating a `Model` class. In this notebook, we focus on the pywatershed-centric instantiation and leave the PRMS-legacy instantiation for another time. \n",
    "\n",
    "With the pywatershed-centric approach, the first argument is a \"model dictionary\" which does nearly all the work (the other arguments will be their default values). The `help()` describes the model dictionary and provides examples. Please use it for reference and more details. Here we'll give an extended concrete example. The `help()` also describes how a `Model` can be instantiated from a model dictionary contained in a YAML file. First, we'll build a model dictionary in memory, then we'll write it out as a yaml file and instantiate our model directly from the YAML file. \n",
    "\n",
    "### Construct the model specification in memory\n",
    "Because our (pre-existing) parameter files (which come with `pywatershed`) and our `Process` classes are consistently named, we can begin to build the model dictionary quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd5038-e0bd-4e8e-9020-00520f18b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "\n",
    "for proc in nhm_processes:\n",
    "    # this is the class name\n",
    "    proc_name = proc.__name__\n",
    "    # the processes can have arbitrary names in the model_dict and\n",
    "    # an instance should not have capitalized name anyway (according to\n",
    "    # python convention), so rename from the class name\n",
    "    proc_rename = \"prms_\" + proc_name[4:].lower()\n",
    "    # each process has a dictionary of information\n",
    "    model_dict[proc_rename] = {}\n",
    "    # alias to shorten lines below\n",
    "    proc_dict = model_dict[proc_rename]\n",
    "    # required key \"class\" specifys the class\n",
    "    proc_dict[\"class\"] = proc\n",
    "    # the \"parameters\" key provides an instance of Parameters\n",
    "    proc_param_file = domain_dir / f\"parameters_{proc_name}.nc\"\n",
    "    proc_dict[\"parameters\"] = pws.Parameters.from_netcdf(proc_param_file)\n",
    "    # the \"dis\" key provides the name of the discretizations\n",
    "    # which we'll supply shortly to the model dictionary\n",
    "    if proc_rename == \"prms_channel\":\n",
    "        proc_dict[\"dis\"] = \"dis_both\"\n",
    "    else:\n",
    "        proc_dict[\"dis\"] = \"dis_hru\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f649ef-2723-44b9-9c49-3f2db85a1010",
   "metadata": {},
   "source": [
    "Let's look at what we have so far in the `model_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57647846-7159-4405-8e46-aab00594c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059641b6-8fbb-49b4-a95f-4e7fd29af3a6",
   "metadata": {},
   "source": [
    "We have given a name to each process and then supplied the class, its parameters, and its discretization for the full set of processes. Now we'll need to add the discretizations to the model dictionary. They are added at the top level and correspond to `dis` the names the processes used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c45a89-b394-4f90-af7d-7bfcf715b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model_dict | {\n",
    "    \"dis_hru\": pws.Parameters.from_netcdf(\n",
    "        domain_dir / \"parameters_dis_hru.nc\"\n",
    "    ),\n",
    "    \"dis_both\": pws.Parameters.from_netcdf(\n",
    "        domain_dir / \"parameters_dis_both.nc\"\n",
    "    ),\n",
    "}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b11ab-4c2c-4671-9127-1578d8a9cfe0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For the time being, `PRMSChannel` needs to know about both HRUs and segments, so `dis_both` is used. We plan to remove this requirement in the near future by implementing \"exchanges\" between processes into the model dictionary.\n",
    "\n",
    "You may have noticed that we are missing a `Control` object to provide time information to the processes. We'll create it and we'll also create a list of the order that the processes are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e38dc-f01c-4ffc-933e-d4c2a901a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = nb_output_dir / \"run_dir\"\n",
    "control = pws.Control(\n",
    "    start_time=np.datetime64(\"1979-01-01T00:00:00\"),\n",
    "    end_time=np.datetime64(\"1980-12-31T00:00:00\"),\n",
    "    time_step=np.timedelta64(24, \"h\"),\n",
    "    options={\n",
    "        \"input_dir\": domain_dir,\n",
    "        \"budget_type\": \"error\",\n",
    "        \"netcdf_output_dir\": run_dir,\n",
    "    },\n",
    ")\n",
    "model_order = [\"prms_\" + proc.__name__[4:].lower() for proc in nhm_processes]\n",
    "model_dict = model_dict | {\"control\": control, \"model_order\": model_order}\n",
    "pprint(model_dict, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a7ac8-c23e-4632-b655-6dc8e884172d",
   "metadata": {},
   "source": [
    "### Instantiate the model\n",
    "\n",
    "The `model_dict` above now specifies a complete model built from multiple processes. Connecting the processes is handled by the `Model` class which can figure it all out because each process fully describes itself (as we saw in the previous notebook), including its inputs and variables. If we instantiate a model from this `model_dict`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba1ed4-fb1b-4fa1-bd38-f47113bdaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pws.Model(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260278e-91ff-47af-a2c1-b9c02e8beaa1",
   "metadata": {},
   "source": [
    "### ModelGraph\n",
    "Now we can examine how the `Processes` are all connected using the `ModelGraph` class. We'll bring in the default color scheme for NHM `Processes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81929f-76d8-470f-9976-7d23f8c1af91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "palette = pws.analysis.utils.colorbrewer.nhm_process_colors(model)\n",
    "pws.analysis.utils.colorbrewer.jupyter_palette(palette)\n",
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        model,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    static_url = \"https://github.com/EC-USGS/pywatershed/releases/download/1.1.0/notebook_01_cell_11_model_graph.png\"\n",
    "    print(\n",
    "        f\"Dot fails on some machines. You can see the graph at this url: {static_url}\"\n",
    "    )\n",
    "    from IPython.display import Image\n",
    "\n",
    "    display(Image(url=static_url, width=1300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ed289-17ba-4925-922d-d8e6f01bbb6b",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* What are the inputs for this model and where are these found? Is there anything special about those files? Could we drive any process from file?\n",
    "* Can you see where each process gets its inputs from in this model? What is the largest number of other processes a single process draws inputs from?\n",
    "* Are some of the arrows 2-way?\n",
    "* Which processes are mass conservative? Can you see the terms involved in mass conservation?\n",
    "* Which process has the greatest/smallest ratio of number of parameters to number of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc28c9b-8d33-479f-b4f8-b8985d86d9c8",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "Now we'll initialize NetCDF output and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d6a26-3aa0-4064-b1ea-117fd2bff6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.run(finalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cd00e-46a5-4778-abc5-f3566f7fe74a",
   "metadata": {},
   "source": [
    "Now we have a finalized run of our model. Finalizing is important mainly so that open output files are closed. We can quite easily look at all the output resulting from our run by looking at the netcdf files in the run directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f305fd-0cfa-4b89-a1fb-c32e72490b8a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "output_files = sorted(run_dir.glob(\"*.nc\"))\n",
    "print(len(output_files))\n",
    "pprint(output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a201df-6a88-4cb3-ada8-95b8f949de89",
   "metadata": {},
   "source": [
    "The following code will let us examine output variables, plotting the full timeseries at individual locations which can be scrolled through using the bar on the right side. It will not work to look at the out budget output files, however. Note, this plot is not a custom plot function. It is base functionality in hvplot (with an xarray backend). Because of all the work getting dimensions and metadata into the NetCDF file, the scroll on the spatial dimension is appropriately named, the y-axis is appropriately labeled with units, and the time axis looks sharp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e4070-6af5-4ff3-921e-fae61e3a8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"seg_outflow\"\n",
    "var_da = xr.load_dataarray(run_dir / f\"{var}.nc\")\n",
    "var_da.hvplot(groupby=var_da.dims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078346e-8b0f-48f0-b7a9-4200714b403b",
   "metadata": {},
   "source": [
    "We'll plot the last variable in the loop, `unused_potet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45454b13-020b-4867-bd34-21c8cdb00834",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "%%do_not_run_this_cell\n",
    "proc_plot = pws.analysis.process_plot.ProcessPlot(gis_files.gis_dir / \"drb_2yr\")\n",
    "proc_classes = [model_dict[nn][\"class\"] for nn in model_order]\n",
    "\n",
    "\n",
    "def get_var_proc_class(var_name):\n",
    "    for proc_class in proc_classes:\n",
    "        if var_name in proc_class.get_variables():\n",
    "            return proc_class\n",
    "\n",
    "\n",
    "proc_plot.plot_hru_var(\n",
    "    var_name=var,\n",
    "    process=get_var_proc_class(var),\n",
    "    data=var_da.mean(dim=\"time\"),\n",
    "    data_units=var_da.attrs[\"units\"],\n",
    "    nhm_id=var_da[\"nhm_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d3fff-4a83-4a4e-a22f-ed5da54412cd",
   "metadata": {},
   "source": [
    "We can also make a spatial plot of the streamflow using a transform for line width representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc3067-9f56-4483-9ce6-d6fe8b1e0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = \"seg_outflow\"\n",
    "# var_da = xr.open_dataarray(run_dir / f\"{var}.nc\")\n",
    "\n",
    "# def xform_width(vals):\n",
    "#     flow_log = np.maximum(np.log(vals + 1.0e-4), 0.0)\n",
    "#     width_max = 5\n",
    "#     width_min = 0.2\n",
    "#     flow_log_lw = (width_max - width_min) * (flow_log - np.min(flow_log)) / (\n",
    "#         np.max(flow_log) - np.min(flow_log)\n",
    "#     ) + width_min\n",
    "#     return flow_log_lw\n",
    "\n",
    "\n",
    "# proc_plot.plot(\n",
    "#     var,\n",
    "#     process=get_var_proc_class(var),\n",
    "#     value_transform=xform_width,\n",
    "#     data=var_da.mean(dim=\"time\"),\n",
    "#     title=f\"{var}\",\n",
    "#     aesthetic_width=True,\n",
    "# )\n",
    "\n",
    "# #proc_plot.plot(var_name, proc, title=var_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae49ade-3c8b-4fe8-af5a-9497f856151b",
   "metadata": {},
   "source": [
    "### Reduce model output to disk\n",
    "Quite a lot of output was written in the above example. In many cases, the amount of model output can be reduced in favor of imporving/reducing model run time. In the next cell, we show how you would reduce the output by setting `control.options['netcdf_output_var_names]`. We'll suppose that we only want the output variables from the `PRMSGroundwater` and `PRMSChannel` processes. Note that we are just combining the variable names returned by these two processes' `.get_variables()` methods. However, we could specify any list of variable names we like (variable names not present in the model are ignored silently, so spelling obviously matters). We dont run this cell, we just show what code you'd change above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d8184-9837-4c01-97db-ea1e53f956ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%do_not_run_this_cell\n",
    "desired_output = [\n",
    "    *pws.PRMSGroundwater.get_variables(),\n",
    "    *pws.PRMSChannel.get_variables(),\n",
    "]control_cp.options[\"netcdf_output_var_names\"] = desired_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420c326-be66-41df-82f1-3c332c1f85ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "When I reduce the original ~150 output files to just those specified in the above cell, run time is reduced by about 60% on my Mac."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3d4c4-8bea-44e2-be76-b274bf04c3d7",
   "metadata": {},
   "source": [
    "## NHM Submodel for the Delaware River Basin \n",
    "In many cases, running the full NHM model may not be necessary and it may be advantageous to just run some of the processes in it. Pywatershed gives you this flexibility. Suppose you wanted to change parameters or model process representation in just the PRMSSoilzone to better predict streamflow. As the model is 1-way coupled, you can simply run a submodel starting with PRMSSoilzone and running through PRMSChannel. \n",
    "\n",
    "In this example we'll construct our model using YAML file, instead of in memory as above. To see how this works, we'll start from a YAML file that specifies the full NHM that we ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19a089-4828-4d80-9b0d-77133a93f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_yaml_file = repo_root_dir / \"test_data/drb_2yr/nhm_model.yaml\"\n",
    "model_dict_yaml = read_yaml(model_dict_yaml_file)\n",
    "display(model_dict_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64163a-339e-4539-bc4f-00adad1d2ae6",
   "metadata": {},
   "source": [
    "We can see above that a YAML file specifies data via a control YAML files and all other data via NetCDF files. All other fields are strings. \n",
    "\n",
    "Let's write our own YAML file for our submodel. Files specified with relative paths are relative the location of the YAML file itself. We want to put this YAML file in to a new run directory, so we'll want to supply paths to existing files and since we dont want/need to copy those, we'll use absolute paths. All `pl.Path`s must be converted to `str`s in the YAML representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e1807-bed4-4c52-99e0-b5a58e63eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_new = {\n",
    "    \"control\": \"nhm_control.yaml\",\n",
    "    \"dis_hru\": str(domain_dir / \"parameters_dis_hru.nc\"),\n",
    "    \"dis_both\": str(domain_dir / \"parameters_dis_both.nc\"),\n",
    "    \"soilzone\": {\n",
    "        \"class\": \"PRMSSoilzone\",\n",
    "        \"parameters\": str(domain_dir / \"parameters_PRMSSoilzone.nc\"),\n",
    "        \"dis\": \"dis_hru\",\n",
    "    },\n",
    "    \"groundwater\": {\n",
    "        \"class\": \"PRMSGroundwater\",\n",
    "        \"parameters\": str(domain_dir / \"parameters_PRMSGroundwater.nc\"),\n",
    "        \"dis\": \"dis_hru\",\n",
    "    },\n",
    "    \"channel\": {\n",
    "        \"class\": \"PRMSChannel\",\n",
    "        \"parameters\": str(domain_dir / \"parameters_PRMSChannel.nc\"),\n",
    "        \"dis\": \"dis_both\",\n",
    "    },\n",
    "    \"model_order\": [\"soilzone\", \"groundwater\", \"channel\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547f876-b323-4955-9394-40abbaba32fe",
   "metadata": {},
   "source": [
    "We'll need to place a control YAML file in our run dir since that's where we said it would be. We'll use a control YAML file that is used for running the full model as a staring point. But will we need to edit it? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0f5d0-42ec-42bc-927b-bf3424568f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_yaml_file = repo_root_dir / \"test_data/drb_2yr/nhm_control.yaml\"\n",
    "model_control_yaml = read_yaml(model_control_yaml_file)\n",
    "display(model_control_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbfc6f-ba55-4826-a755-4bba96e2cbe9",
   "metadata": {},
   "source": [
    "Looking closely at this, we'll notice that `input_dir` is not specified. Trying to instantiate a model will throw an error telling us this. But where do we get our inputs? What are our inputs? What were our inputs above? Maybe, let's try that same directory we used for the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9283a-491e-4a87-83de-0df354ed1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_yaml[\"input_dir\"] = str(control.options[\"input_dir\"])\n",
    "run_dir_submodel = nb_output_dir / \"run_dir_submodel\"\n",
    "run_dir_submodel.mkdir(exist_ok=True)\n",
    "model_control_yaml[\"netcdf_output_dir\"] = str(run_dir_submodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81e3de-a1be-4bb1-93ae-b95090dedb42",
   "metadata": {},
   "source": [
    "Now let's write out our model and control YAML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da14c3-6571-4db6-a263-6fa8afd5e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_yaml_file = run_dir_submodel / \"submodel.yaml\"\n",
    "write_yaml(model_dict_new, submodel_yaml_file)\n",
    "write_yaml(\n",
    "    model_control_yaml, run_dir_submodel / \"nhm_control.yaml\"\n",
    ")  # as specified in model_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c502d9b-3074-4a1d-ac3f-624f19c6419d",
   "metadata": {},
   "source": [
    "We'll run the model from YAML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963ea8b-8385-46f9-860f-2cfff55564d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    submodel = pws.Model.from_yaml(submodel_yaml_file)\n",
    "except Exception as error:\n",
    "    print(\"An exception occurred:\", error)  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff220a-95bf-4044-9d77-76d1d1db1b0d",
   "metadata": {},
   "source": [
    "We got an error that the `potet.nc` file was not found. What is going on? Why is that an input file? Let's take a look at the `ModelGraph` for this submodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06dd22-e240-4b17-b60f-0a1fb42dca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_params = not (platform == \"darwin\" and processor() == \"arm\")\n",
    "try:\n",
    "    pws.analysis.ModelGraph(\n",
    "        submodel,\n",
    "        hide_variables=False,\n",
    "        process_colors=palette,\n",
    "        show_params=show_params,\n",
    "    ).SVG(verbose=True, dpi=48)\n",
    "except:\n",
    "    static_url = \"https://github.com/EC-USGS/pywatershed/releases/download/1.1.0/notebook_01_cell_45_submodel_graph.png\"\n",
    "    print(\n",
    "        f\"Dot fails on some machines. You can see the graph at this url: {static_url}\"\n",
    "    )\n",
    "    from IPython.display import Image\n",
    "\n",
    "    display(Image(url=static_url, width=700))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367045a-34f8-4541-8f78-ea886e6fe29e",
   "metadata": {},
   "source": [
    "OK, the submodel has a different set of inputs that the `ModelGraph` clearly shows. That's cool, but where will we find those files? Remember when we ran the full model above? Maybe it output the required inputs? How could we check this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd7229-49ad-4bce-b097-930660f8d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = [\n",
    "    *pws.PRMSSoilzone.get_inputs(),\n",
    "    *pws.PRMSRunoff.get_inputs(),\n",
    "    *pws.PRMSChannel.get_inputs(),\n",
    "]\n",
    "all_run_output_names = [ff.name[0:-3] for ff in sorted(run_dir.glob(\"*.nc\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ed33b-9596-459a-8e5d-6cfa8bacd144",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(all_inputs).difference(set(all_run_output_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24535f1-3293-491e-b681-9ff2df29e2cd",
   "metadata": {},
   "source": [
    "Oh snap! All the inputs files are available from the first run. Let's fix our control's `input_dir`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87269149-5430-4326-8406-8b07cd446755",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_control_yaml[\"input_dir\"] = str(run_dir.resolve())\n",
    "write_yaml(\n",
    "    model_control_yaml, run_dir_submodel / \"nhm_control.yaml\"\n",
    ")  # as specified in model_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4af3a-252a-4c5b-8f0a-41e73f039937",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = pws.Model.from_yaml(submodel_yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96978bd-9c0b-4fae-b035-4565f9f4ce58",
   "metadata": {},
   "source": [
    "The model instantiated just fine. While we could just do `submodel.run(finalize=True)`, that'd be too easy. Let's write the expansion of the run loop implemented under the hood of the `Model` class so you can see how you might explore the internals of the a `Model` instance. You can see some basics of the relationship of a `Model` to its `Processes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9410e6-6b64-420f-8940-d743286ce287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "submodel.initialize_netcdf()\n",
    "for tt in tqdm(range(control.n_times)):\n",
    "    submodel.control.advance()\n",
    "    for cls in submodel.process_order:\n",
    "        submodel.processes[cls].advance()\n",
    "        submodel.processes[cls].calculate(1.0)\n",
    "        submodel.processes[cls].output()\n",
    "\n",
    "submodel.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787a163-c4dd-4826-b0f2-73e1b006c081",
   "metadata": {},
   "source": [
    "Well, the submodel saved us some time. Again, about 60% of the original run time (like when reducing the number of output variables). Below, we'll show that the submodel run is identical to the original run, for the processes included. \n",
    "\n",
    "First, let's lookat the internals of the `submodel in a bit more detail. The final time is still in memory so we can take a closer look at, say, recharge. We'll look at its metadata, its dimensions, shape, type, and dtype in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf15be-871b-4b58-ad1f-45bfcb37fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(pws.meta.find_variables(\"recharge\"))\n",
    "print(\n",
    "    \"PRMSSoilzone dimension names: \",\n",
    "    submodel.processes[\"soilzone\"].dimensions,\n",
    ")\n",
    "print(\"nhru: \", submodel.processes[\"soilzone\"].nhru)\n",
    "print(\n",
    "    \"PRMSSoilzone recharge shape: \",\n",
    "    submodel.processes[\"soilzone\"][\"recharge\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge type: \",\n",
    "    type(submodel.processes[\"soilzone\"][\"recharge\"]),\n",
    ")\n",
    "print(\n",
    "    \"PRMSSoilzone recharge dtype: \",\n",
    "    submodel.processes[\"soilzone\"][\"recharge\"].dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af149db1-970c-47d2-b9dd-0d0b21f80810",
   "metadata": {},
   "source": [
    "We see the length of the `nhru` dimension and that this is the only dimension on `recharge`. With the exception of the `PRMSSolar` and `PRMSAtmosphere` classes (which vectorizes compuations over time), `Processes` only have spatial dimensions. Their data is written to file with each timestep. Prognostic variables have a `variable_previous` (or `_old` or `_ante`, etc) version to store the antecedent values. One design feature of pywatershed is that all such prognostic variables can be identified in a `Process`'s `.advance()` method. \n",
    "\n",
    "FOr our current `submodel`, the last timestep is still in memory (even though we've finalized the run) and we can visualize it. The data are on the unstructured/polygon grid of Hydrologic Response Units (HRUs), we'll visualize the spatial distribution at this final time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac5832-7437-467e-901d-c40e2d9ddab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%do_not_run_this_cell\n",
    "proc_plot = pws.analysis.process_plot.ProcessPlot(gis_files.gis_dir / \"drb_2yr\")\n",
    "proc_name = \"soilzone\"\n",
    "var_name = \"ssr_to_gw\"\n",
    "proc = submodel.processes[proc_name]\n",
    "display(proc_plot.plot(var_name, proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13500c68-c363-4f8b-8db0-2c069aa4c5d7",
   "metadata": {},
   "source": [
    "We can easily check the results of our submodel model against our full model. This gives us an opportunity to look at the output files. We can start with recharge as our variable of interest. The model NetCDF output can be read in using `xarray` where we can see all the relevant metadata quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eb7ef-c6e1-4ecf-b475-59d8552117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"recharge\"\n",
    "nhm_da = xr.load_dataarray(run_dir_submodel / f\"{var}.nc\")\n",
    "sub_da = xr.load_dataarray(run_dir / f\"{var}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7b2e0-c9b6-4100-a328-42add2cbc4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nhm_da)\n",
    "display(sub_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f317363-c765-4dda-a972-92f5265abd47",
   "metadata": {},
   "source": [
    "Now we can compare all output variables common to both runs, asserting that the two runs gave equal output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92734cb1-2a69-4d4e-a575-692fef9d5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_variables = [\n",
    "    *pws.PRMSSoilzone.get_variables(),\n",
    "    *pws.PRMSGroundwater.get_variables(),\n",
    "    *pws.PRMSChannel.get_variables(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfeb46f-8d29-49f4-8945-f9b443831f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in submodel_variables:\n",
    "    nhm_da = xr.load_dataarray(run_dir / f\"{var}.nc\")\n",
    "    sub_da = xr.load_dataarray(run_dir_submodel / f\"{var}.nc\")\n",
    "    xr.testing.assert_equal(nhm_da, sub_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06a8de-c605-409a-b09d-a42674fab66c",
   "metadata": {},
   "source": [
    "We can make some scatter plots and timeseries plots for any variable of interest, since you were not convinced by the `assert_equal` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ed710-d23a-4c80-8b28-0a1612a636b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = \"seg_outflow\"\n",
    "nhm_da = xr.load_dataarray(run_dir / f\"{var_name}.nc\")\n",
    "sub_da = xr.load_dataarray(run_dir_submodel / f\"{var_name}.nc\")\n",
    "scat = xr.merge(\n",
    "    [nhm_da.rename(f\"{var_name}_yaml\"), sub_da.rename(f\"{var_name}_subset\")]\n",
    ")\n",
    "space_dim = sub_da.dims[1]\n",
    "display(\n",
    "    scat.hvplot(\n",
    "        x=f\"{var_name}_yaml\", y=f\"{var_name}_subset\", groupby=space_dim\n",
    "    ).opts(data_aspect=1)\n",
    ")\n",
    "\n",
    "scat.hvplot(y=f\"{var_name}_subset\", groupby=space_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf31415-8cc9-4688-bf28-08c905990433",
   "metadata": {},
   "source": [
    "### Adapter class\n",
    "The `Adapter` class is the bit of magic behind how we drive `Processes` from files or from other `Processes`. Here we'll give a quick demo of the how this class works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de8018-f152-459a-8fde-10a8ce3c56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pws.Control.from_yaml(run_dir_submodel / \"nhm_control.yaml\")\n",
    "recharge_adapter = pws.adapter_factory(\n",
    "    run_dir_submodel / \"recharge.nc\", \"recharge\", control\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2ecdc-ed3b-4fad-a185-63b03cc3228b",
   "metadata": {},
   "source": [
    "Before the control and the adapter are advanced in time, the adapter has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55850d5e-d82d-4c1e-94b7-0bf3aabda086",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_adapter.current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8890f-c679-4d9f-856e-a10c5dd347e3",
   "metadata": {},
   "source": [
    "We advance through all time and we'll check that we get the values that are still in memory. This demo shows how the adapter class can easily make a NetCDF file look like a `Process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc29fb5-d73f-4fe9-b2b6-76d5079a4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tt in range(control.n_times):\n",
    "    control.advance()\n",
    "    recharge_adapter.advance()\n",
    "    if tt == 0:\n",
    "        display(recharge_adapter.current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84140cc-232c-4bca-aa91-0e45524b2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(recharge_adapter.current == submodel.processes[\"soilzone\"][\"recharge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91401478-34ab-4277-804e-6ee2d4403d52",
   "metadata": {},
   "source": [
    "## References\n",
    "* Regan, R. S., Markstrom, S. L., Hay, L. E., Viger, R. J., Norton, P. A., Driscoll, J. M., & LaFontaine, J. H. (2018). Description of the national hydrologic model for use with the precipitation-runoff modeling system (prms) (No. 6-B9). US Geological Survey.\n",
    "* Regan, R.S., Markstrom, S.L., LaFontaine, J.H., 2022, PRMS version 5.2.1: Precipitation-Runoff Modeling System (PRMS): U.S. Geological Survey Software Release, 02/10/2022."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
